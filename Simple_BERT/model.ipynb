{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment analysis using BERT\n"
      ],
      "metadata": {
        "id": "8r2OXtOFHoe8"
      },
      "id": "8r2OXtOFHoe8"
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "_8teYD6TGt8C"
      },
      "id": "_8teYD6TGt8C",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading dataset"
      ],
      "metadata": {
        "id": "mp_8O_NNHcnH"
      },
      "id": "mp_8O_NNHcnH"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "path = tf.keras.utils.get_file(\n",
        "    \"aclImdb_v1.tar.gz\",\n",
        "    url,\n",
        "    untar=True,\n",
        "    cache_dir='.',\n",
        "    cache_subdir=''\n",
        "    )\n",
        "\n",
        "dataset = os.path.join(os.path.dirname(path), 'aclImdb')\n",
        "\n",
        "train_dataset = os.path.join(dataset, 'train')\n",
        "test_dataset = os.path.join(dataset, 'test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyCFgAs0HX0H",
        "outputId": "f023ab1e-02cd-48ff-df15-b787b74581d3"
      },
      "id": "QyCFgAs0HX0H",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "\u001b[1m84125825/84125825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/aclImdb_v1_extracted/\"\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    path + 'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "class_name = raw_train_ds.class_names\n",
        "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    path + 'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    path + 'aclImdb/test',\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYhiAaaRInAT",
        "outputId": "c99f8e2d-b58c-4fc1-b00d-a7a4d78731c3"
      },
      "id": "uYhiAaaRInAT",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 75000 files belonging to 3 classes.\n",
            "Using 60000 files for training.\n",
            "Found 75000 files belonging to 3 classes.\n",
            "Using 15000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t, l in train_ds.take(1):\n",
        "  for i in range(10):\n",
        "    print(f\"Review: {t.numpy()[i]}\\nLabel: {l.numpy()[i]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-K03qbjKHBD",
        "outputId": "acdb8e05-f679-447e-e4ea-23533c2cd38c"
      },
      "id": "w-K03qbjKHBD",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: b'There is this father-son conversation in the climax of \\'KALPURUSH\\'. I quote the English DVD-subtitle version. Shumonto tells his father: \"I may not have become someone, but when I see two people in love, I smile. And when I see someone eating alone, I cry.\" Ashvini, his father, replies wistfully: \"I wish I could\\'ve lived my life like you did.\" These 2 lines, perhaps, comprise the gist of this new film by Buddhadev Dasgupta - director of teeny-weeny gems like \\'Tahader Katha\\', \\'Bagh Bahadur\\', \\'Uttara\\' & \\'Mondo Meyer Upakhyan\\' - which took nearly 3 years to reach the cinemas in India.<br /><br />The film opens with a man called Ashvini following a younger man called Shumonto, who, we are told, is his son. It seems that the father is stalking - or haunting, rather - his son. As the film progresses and we meet Shumonto\\'s ambitious wife, Supriya, and his mother, Koyel, who seems to be tied up with something in her past, we realise that the son is, indeed, haunted by his father who was a somebody. He was a successful doctor and they had this beautiful family, but something - or someone - comes in and this happy husband-wife-child drift apart. This drifting apart is too hard for these three to endure, and the son, we see, is unable to lead even a proper relationship with his wife.<br /><br />I don\\'t know of too many father-son films from Bollywood or other Indian language films. I\\'ve seen only Ramesh Sippy\\'s \\'Shakti\\' & Feroze A. Khan\\'s \\'Gandhi, my father\\'. Both were the powerful types with dramatic, sad endings. I\\'ve also seen \\'Thevar Magan\\' & its Hindi remake \\'Virasat\\', but they were different. KALPURUSH is drama, but not the powerful type. It is, like other Dasgupta films, creatively- photographed, dream-like, poetic & soft. This keeps the viewer absolutely un-prepared for the surprise ending, for the film hardly feels like a father-son film. It starts like a husband-wife story, creates the tension, goes to become a mother-son film, and then explains why it is so, why the son is so, what was his relationship like with his father, what happens to the father, the mother, what the son\\'s wife does, and how the son carries on with his life.<br /><br />It would be unfair to dub KALPURUSH strictly for Buddhadev Dasgupta fans. However, I do suggest that the viewers acquaint themselves with Dasgupta\\'s films before going to see KALPURUSH. Dasgupta\\'s films are often accused of having a near-invisible storyline. KALPURUSH is no different. It starts, too, in a very un-Buddhadev Dasgupta-ish way. Instead of bare landscapes of Puruliya & Midnapore, one sees the trams of Calcutta in the opening credits. It helps, though, for it is like - What is this, urban Bengal? Soon after this, the film turns typical Dasgupta. The rural Bengal, this time, are the scenic outdoor locales of coastal Orissa.<br /><br />The actors are stupendous. Mithun Chakraborty is a legend. This is his second film with Dasgupta. He plays his age, suits the part, one just has to see him in this one. Rahul Bose is so silent one could feel the loss of his loser character. Sameera Reddy has looked good in just 3 films - \\'Musafir\\', \\'Migration\\' & \\'Kalpurush\\'. I haven\\'t seen \\'Ami, Yasin ar amar Madhubala\\' so I can\\'t comment on that. She better shift to the Bengali film industry. A mentor like Dasgupta would surely do her a lot good. Sudipta Chakraborty\\'s Other Woman role is short, but long enough to bring in that right amount of glamour, desire & heartbreak. Labony Sarkar is natural with a capital N.<br /><br />With the usual Buddhadev Dasgupta tropes in place, KALPURUSH is a visual treat. There are bare landscapes, dry leaves flying in the wind, haunting background score, mysterious folk artistes in even more mysterious costumes & masks, dilapidated, old buildings, and things rustic and antique. This time there is also the sea and an aeroplane flying right outside the open window. KALPURUSH is a film which needs to be seen.'\n",
            "Label: 2\n",
            "\n",
            "Review: b'The year 2005 saw no fewer than 3 filmed productions of H. G. Wells\\' great novel, \"War of the Worlds\". This is perhaps the least well-known and very probably the best of them. No other version of WotW has ever attempted not only to present the story very much as Wells wrote it, but also to create the atmosphere of the time in which it was supposed to take place: the last year of the 19th Century, 1900 \\xc2\\x85 using Wells\\' original setting, in and near Woking, England.<br /><br />IMDb seems unfriendly to what they regard as \"spoilers\". That might apply with some films, where the ending might actually be a surprise, but with regard to one of the most famous novels in the world, it seems positively silly. I have no sympathy for people who have neglected to read one of the seminal works in English literature, so let\\'s get right to the chase. The aliens are destroyed through catching an Earth disease, against which they have no immunity. If that\\'s a spoiler, so be it; after a book and 3 other films (including the 1953 classic), you ought to know how this ends.<br /><br />This film, which follows Wells\\' plot in the main, is also very cleverly presented \\xc2\\x96 in a way that might put many viewers off due to their ignorance of late 19th/early 20th Century photography. Although filmed in a widescreen aspect, the film goes to some lengths to give an impression of contemporaneity. The general coloration of skin and clothes display a sepia tint often found in old photographs (rather than black). Colors are often reminiscent of hand-tinting. At other times, colors are washed out. These variations are typical of early films, which didn\\'t use standardized celluloid stock and therefore presented a good many changes in print quality, even going from black/white to sepia/white to blue/white to reddish/white and so on \\xc2\\x96 as you\\'ll see on occasion here. The special effects are deliberately retrograde, of a sort seen even as late as the 1920s \\xc2\\x96 and yet the Martians and their machines are very much as Wells described them and have a more nearly realistic \"feel\". Some of effects are really awkward \\xc2\\x96 such as the destruction of Big Ben. The acting is often more in the style of that period than ours. Some aspects of Victorian dress may appear odd, particularly the use of pomade or brilliantine on head and facial hair.<br /><br />This film is the only one that follows with some closeness Wells\\' original narrative \\xc2\\x96 as has been noted. Viewers may find it informative to note plot details that appear here that are occasionally retained in other versions of the story. Wells\\' description of the Martians \\xc2\\x96 a giant head mounted on numerous tentacles \\xc2\\x96 is effectively portrayed. When the Martian machines appear, about an hour into the film, they too give a good impression of how Wells described them. Both Wells and this film do an excellent job of portraying the progress of the Martians from the limited perspective (primarily) of rural England \\xc2\\x96 plus a few scenes in London (involving the Narrator\\'s brother). The director is unable to resist showing the destruction of a major landmark (Big Ben), but at least doesn\\'t dwell unduly on the devastation of London.<br /><br />The victory of the Martians is hardly a surprise, despite the destruction by cannon of some of their machines. The Narrator, traveling about to seek escape, sees much of what Wells terms \"the rout of Mankind\". He encounters a curate endowed with the Victorian affliction of a much too precious and nervous personality. They eventually find themselves on the very edge of a Martian nest, where they discover an awful fact: the Martians are shown to be vampires who consume their prey alive in a very effective scene. Wells adds that after eating they set up \"a prolonged and cheerful hooting\". The Narrator finally is obliged to beat senseless the increasingly hysterical curate \\xc2\\x96 who revives just as the Martians drag him off to the larder (cheers from the gallery; British curates are so often utterly insufferable).<br /><br />This film lasts almost 3 hours, going through Wells\\' story in welcome detail. It\\'s about time the author got his due \\xc2\\x96 in a compelling presentation that builds in dramatic impact. A word about the acting: Don\\'t expect award-winning performances. They\\'re not bad, however, the actors are earnest and they grow on you. Most of them, however, have had very abbreviated film careers, often only in this film. The Narrator is played by hunky Anthony Piana, in his 2nd film. The Curate is John Kaufman \\xc2\\x96 also in his 2nd film as an actor but who has had more experience directing. The Brother (\"Henderson\") is played with some conviction by W. Bernard Bauman in his first film. The Artilleryman, the only other sizable part, is played by James Lathrop in his first film.<br /><br />This is overall a splendid film, portraying for the first time the War of the Worlds as Wells wrote it. Despite its slight defects, it is far and away better than any of its hyped-up competitors. If you want to see H. G. Wells\\' War of the Worlds \\xc2\\x96 and not some wholly distorted version of it \\xc2\\x96 see this film!'\n",
            "Label: 1\n",
            "\n",
            "Review: b\"This show is probably one of the worst shows I've seen on the network. I have begun to even avoid its commercials because I still am astonished how its been on TV so long. This show is probably the worst thing I've seen in a decade. When I say bad... I MEAN BAD. The things done are mostly scripted and very repetitive. All there is, is riots and people beating other people up. I just don't understand how this show was even thought about putting it on air, because its just so bad. It's completely ridiculous. Out of 5 stars *****, I don't think I would even give it a star.<br /><br />At all costs. Avoid this show.\"\n",
            "Label: 2\n",
            "\n",
            "Review: b\"I wish I could find something good about this film but helas... I really tried hard watching it at different times but still, it's almost unbearable to watch. I really envy non french-speakers who may then not be affected by the terrible acting (but the script is basically awful so the actors may not be totally responsible). This film is discontinued, the storyline is either too slow or too fast, no identification with the characters is possible, it's just uncanny. The actors seem to 'recitate' their part with no emotion (Deneuve has such a monotonous and unconvincing tone of voice. Vincent Perez is very good-looking but truly has no talent whatsoever). Everything happens abruptly with no real continuity, the editing is appalling. We don't see that much of the Vietnam either and these bits of history do nothing for the film. What this film lacks above all is depth, it just goes in different directions with no coherence. Bits of this and bits of that, I find HARD to believe this film got an award for the best foreign film. The landscapes maybe? I certainly would recommend 'The Lover' inspired by Marguerite Duras' novel of the same name as well as films like 'Cyclo' or 'the scent of the green papaya'. A different aspect of the Vietnam but far more interesting.\"\n",
            "Label: 2\n",
            "\n",
            "Review: b\"I don't understand people writing here this movie is a soap opera. OK it's about relationships and it's not a character sketch, but it's also a well done movie with some very strong moments and some fine details.\"\n",
            "Label: 2\n",
            "\n",
            "Review: b'Warning Spoiler. . . I have to agree with you, it was almost there. This was such a bad movie, about such and interesting true story. It had such promise, but the acting was ridiculous at best. Some sets were beautiful and realistic. Others are something out of a theme park. I found myself laughing as I watched, what was suppose to be, serious scenes. I really wanted to like this movie, but I couldn\\'t. The best part was the fight between friends that ended with the \"King\" dying. I liked the Queens\\' punishment. And, the final shot made a beautiful picture, though. There are so many better movies to watch. I don\\'t recommend this.'\n",
            "Label: 0\n",
            "\n",
            "Review: b\"Before watching this movie I had some serious doubts about it. Not only is this a courtroom drama (and as you know the streets of Hollywood seem to be paved with this kind of scripts), it also featured Cher as one of the main actresses. I'm not really a fan of her as a singer, but seeing her as a good actress is even a lot harder. As you know, almost all pop diva's, young or old, seem to have that urge to appear in one or two movies and that almost always results in complete disasters. So why would Cher be any different...? <br /><br />When a judge commits suicide and his secretary is found murdered in a river, a homeless and deaf-mute man, named Carl Anderson, is arrested for her murder, because all indirect evidence points to him. Because he can't afford a lawyer, public defender Kathleen Riley is assigned by the court as his lawyer. Even though she doesn't always believes in his innocence, she still goes after the real killer. She gets help from the congressional adviser Eddie Sanger, who is called to be on the jury panel and together they find some important evidence that the murder has something to do with corruption in some high ranks...<br /><br />I must admit that Cher has done a better job than I ever expected from her. She actually was very convincing and interesting to watch as the public defender. Together with Liam Neeson she makes this movie work. Their nice performances and their difficult professional relationship in this movie are actually the best thing this movie has to offer. The story on itself certainly isn't that bad, but the plot is a bit far-fetched and gives this movie an ending that is a bit too abrupt.<br /><br />In the end this is a reasonably well-done courtroom drama / thriller that lacks the required tension to be fully satisfying, but which offers some nice acting and some good direction. It's not the best movie in the genre, but it is enjoyable enough to be worth a watch. I give it a 6.5/10.\"\n",
            "Label: 2\n",
            "\n",
            "Review: b'The daredevils/heroes are: Gene-- the brains of the outfit, Tiny \\xc2\\x96 a gifted strongman, and Bert -- an agile escape artist. They are aided by Carole Landis who plays Blanche Grandville \\xc2\\x96 the granddaughter of Horace Grandville whose multimillion dollar industrial complex is under attack by master criminal 39013. This film is twelve chapters of action packed cliff-hanging fun. <br /><br />I particularly liked the fact that these circa 1939 daredevils did almost all of their action scenes in double breasted three piece suits! <br /><br />The history of the major actors is quite interesting: <br /><br />Dave Sharpe,Bert, became an Army pilot during WWII and a major stuntman; having a long and celebrated career in the industry. He has been ranked with the great Yakima Canutt. <br /><br />Charles Quigley, Gene, had a good career but died of cirrhosis of the liver before he turned sixty. <br /><br />Bruce Bennet aka Herman Brix, who played Tiny, was a 1928 Olympic champion who went on to a very long career and lived to be over 100 years old. <br /><br />Carole Landis was 29 when she committed suicide. <br /><br />I bought this film because of the multitude of glowing reviews found on this forum. I was not disappointed!'\n",
            "Label: 2\n",
            "\n",
            "Review: b\"Neil Labute's in the company of men is a misogynistic look at the way men has to conquer women in and out of the bedroom. Aaron Eckhart plays a womanizer who makes a bet with a fellow co-worker to seduce a young deaf woman, and dump her in order to feed their misogynistic egos. In their pursuit of this innocent woman, they wine and dine her, and ultimately humiliate her in such a way that can only be describes as inhuman. Neil Labute's psychological masterpiece is a very sobering look at the game both sexes play, and the cruelty that we can inflict on ourselves.<br /><br />Prepare to be disturbed.\"\n",
            "Label: 2\n",
            "\n",
            "Review: b'Fans of late night 1950\\'s science fiction (insert Cleveland\\'s \"Goulardi\" show here) remember this not as \"Creature with the Atom Brain\" but as the movie with Uncle Dave. John Launer (in his first movie role) played police Capt. Dave Harr, whose niece was seven year old Penny Walker (Linda Bennett). Care is taken early to show that Penny adores her Uncle Dave and late in the story (in the most famous scene in the whole 50\\'s sci-fi genre) he visits her on her birthday. But it is the re-animated Uncle Dave, whose changed voice Mrs. Walker credits to a cold. She leaves Penny in the living room with Dave and goes into the kitchen. Penny then gives Dave her doll Henrietta. Cut to Mrs. Edwards startled face when she hears Penny screaming in the living room. Poor Henrietta!<br /><br />Launer pretty much steals the film from the rest of the cast, both before and after he is killed. As Captain Harr he cracks wise in every scene and his dialogue is actually extremely funny. <br /><br />The premise, to quote Penny\\'s father (Richard Denning): \"Remote-controlled creatures, their brains powered by atomic energy, roaming the streets, directed from a central point\". <br /><br />The background involves the deportation of Frank Buchanan (Michael Granger), an American gangster who vows revenge and finances a not so mad German scientist (Gregory Gaye) who is developing a process (based on then trendy radioactivity) to animate dead bodies. They set up a laboratory in the lead lined walls of a LA mansion, steal a few bodies from the morgue, and begin a cottage industry of wiping out Buchanan\\'s list of enemies. <br /><br />The scientist spends most of his screen time complaining about the corruption of his noble experiment for evil means. The set design for the lab is unexpectedly elaborate and the sequences of the two men crawling around in a 1950\\'s idea of radiation suits are especially effective. <br /><br />One of their creatures in the climatic scene looks a lot like Treat Williams. This scene has a \"Night of the Living Dead\" flavor. Although the authorities have discovered that the bodies are controlled by electrodes surgically planted in their brains, they conveniently fail to mention this to the police and national guardsmen who are fighting the monsters; allowing the climatic scene to continue until other means are found to deactivate the creatures. Up until this scene, the movie deliberately used almost entirely wide master shots, much like the \"mise en scene\" (limited editing cuts within a scene) technique of some Hitchcock films. Probably done for budget reasons, it serves nicely to set up the climax, which is full of sudden cuts to extreme close-ups of the walking dead. This must have been quite frightening on the big screen, particularly after being accustomed to the more distanced style of the first sixty minutes of the film.<br /><br />I suggest that potential viewers ignore the IMDb voting, this is a scary horror/sci-fi film done with a trace of self-parody. Another one of Producer Sam Katzman\\'s money making B-movies; where he exploits the growing fear of a nation learning to deal with the atomic age, and its fascination with the concept of radioactivity.'\n",
            "Label: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine tuning a BERT model"
      ],
      "metadata": {
        "id": "M7cgGCqMLBuH"
      },
      "id": "M7cgGCqMLBuH"
    },
    {
      "cell_type": "code",
      "source": [
        "encoder =  ( \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\" )\n",
        "\n",
        "preprocessor =  (\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\" )\n",
        "\n",
        "print(f\"Encoder available: {encoder}\")\n",
        "print(f\"Preprocessor available: {preprocessor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7F1crLAKqZy",
        "outputId": "af9ad5fa-7175-4f6e-c991-94a5e62a3f77"
      },
      "id": "F7F1crLAKqZy",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder available: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Preprocessor available: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess = hub.KerasLayer(preprocessor)"
      ],
      "metadata": {
        "id": "Nb922XJLYeQ2"
      },
      "id": "Nb922XJLYeQ2",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_text = [\"this is a sample text.\"]\n",
        "testing_text_preprocessed = bert_preprocess(testing_text)\n",
        "\n",
        "print(f\"Keys of testing_text_preprocessed: {testing_text_preprocessed.keys()}\")\n",
        "print(f\"Value of input_word_ids: {testing_text_preprocessed['input_word_ids']}\")\n",
        "print(f\"Value of input_mask: {testing_text_preprocessed['input_mask']}\")\n",
        "print(f\"Value of input_type_ids: {testing_text_preprocessed['input_type_ids']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INjMXopCZDrW",
        "outputId": "25fd6821-202b-44cd-97d3-58cc05274603"
      },
      "id": "INjMXopCZDrW",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys of testing_text_preprocessed: dict_keys(['input_type_ids', 'input_mask', 'input_word_ids'])\n",
            "Value of input_word_ids: [[ 101 2023 2003 1037 7099 3793 1012  102    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "Value of input_mask: [[1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "Value of input_type_ids: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing outputs\n",
        "This BERT model outputs a dictionary containing:\n",
        "\n",
        "*   input_word_ids : token number\n",
        "*   input_mask     : indexing bolean\n",
        "*   input_type_ids :\n",
        "\n"
      ],
      "metadata": {
        "id": "svGhVdfRZDO0"
      },
      "id": "svGhVdfRZDO0"
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = hub.KerasLayer(encoder)"
      ],
      "metadata": {
        "id": "XUTI65V5a1gs"
      },
      "id": "XUTI65V5a1gs",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier(dropout_rate=0.1):\n",
        "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text\")\n",
        "    preprocessing_layer = hub.KerasLayer(\n",
        "        preprocessor, name=\"preprocessing\"\n",
        "    )\n",
        "    encoder_inputs = preprocessing_layer(text_input)\n",
        "    encoder = hub.KerasLayer(\n",
        "        encoder, trainable=True, name=\"BERT_encoder\"\n",
        "    )\n",
        "    outputs = encoder(encoder_inputs)\n",
        "    net = outputs[\"pooled_output\"]\n",
        "    net = tf.keras.layers.Dropout(dropout_rate)(net)\n",
        "    net = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"classifier\")(net)\n",
        "    return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "3rcXyM3nbALK"
      },
      "id": "3rcXyM3nbALK",
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "metadata": {
        "id": "w-ENc2DUcIgT"
      },
      "id": "w-ENc2DUcIgT",
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e10794ec"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "module_path = '/content/models'\n",
        "\n",
        "# Add the directory to sys.path\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "import official.nlp\n"
      ],
      "id": "e10794ec",
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 0.1\n",
        "\n",
        "optimizer = official.nlp.optimization.create_optimizer(\n",
        "    init_lr=init_lr,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    optimizer_type='adamw'\n",
        ")"
      ],
      "metadata": {
        "id": "n8PfXzT7c46n"
      },
      "id": "n8PfXzT7c46n",
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_classifier()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "90iz3qfkdXnt",
        "outputId": "09ed71c3-8260-4992-d75d-cc94ba0c482a"
      },
      "id": "90iz3qfkdXnt",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling layer 'preprocessing' (type KerasLayer).\n\nA KerasTensor is symbolic: it's a placeholder for a shape an a dtype. It doesn't have any actual numerical value. You cannot convert it to a NumPy array.\n\nCall arguments received by layer 'preprocessing' (type KerasLayer):\n  • inputs=<KerasTensor shape=(None,), dtype=string, sparse=False, name=text>\n  • training=None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-88-3539874372.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-84-519260820.py\u001b[0m in \u001b[0;36mbuild_classifier\u001b[0;34m(dropout_rate)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mpreprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"preprocessing\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mencoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     encoder = hub.KerasLayer(\n\u001b[1;32m      8\u001b[0m         \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BERT_encoder\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Behave like BatchNormalization. (Dropout is different, b/181839368.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m       result = smart_cond.smart_cond(training,\n\u001b[0m\u001b[1;32m    251\u001b[0m                                      \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                                      lambda: f(training=False))\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m       result = smart_cond.smart_cond(training,\n\u001b[1;32m    251\u001b[0m                                      \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                                      lambda: f(training=False))\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Unwrap dicts returned by signatures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36mcanonicalize_to_monomorphic\u001b[0;34m(args, kwargs, default_values, capture_types, polymorphic_type)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"self\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m       \u001b[0;31m# Type constraints do not apply on them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m       \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitized_kind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36m_make_validated_mono_param\u001b[0;34m(name, value, kind, type_context, poly_type)\u001b[0m\n\u001b[1;32m    520\u001b[0m     poly_bound_arguments = inspect.BoundArguments(\n\u001b[1;32m    521\u001b[0m         \u001b[0mpoly_bound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_values_injected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m     )\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m   \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mndarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTENSOR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/keras_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'preprocessing' (type KerasLayer).\n\nA KerasTensor is symbolic: it's a placeholder for a shape an a dtype. It doesn't have any actual numerical value. You cannot convert it to a NumPy array.\n\nCall arguments received by layer 'preprocessing' (type KerasLayer):\n  • inputs=<KerasTensor shape=(None,), dtype=string, sparse=False, name=text>\n  • training=None"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}